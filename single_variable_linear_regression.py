# -*- coding: utf-8 -*-
"""Single Variable Linear Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18_aBN6d0qARQHlqF_3aGOc4iIbo0weal
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/exp-1_train.csv')

data.shape

data.head()
data.describe()
data.info()

data.isnull().sum()

data.isnull().sum()/data.shape[0]*100

x=data.iloc[0:700,0:1]
y=data.iloc[0:700,1:2]

x

x.boxplot(column=['x'])

y.boxplot(column=['y'])

import matplotlib.pyplot as plt
#plot the scatter plot
plt.scatter(x,y)
plt.xlabel('x')
plt.ylabel('y')
plt.title("X vs Y Scatter PLOT")

#Linear Regression
def hypothesis(theta_array,x):
  return theta_array[0]+theta_array[1]*x

def cost_function(theta_array,x,y,m):
  error=0
  for i in range(m):
    error=error+((theta_array[0]+theta_array[1]*x[i])-y[i])**2

  return error/(2*m)
  # return (1/(2*m))*np.sum((hypothesis(theta_array,x)-y)**2)


def gradient_descent(theta_array,x,y,m,alpha):
  summation_0=0
  summation_1=0
  for i in range(m):
    summation_0=summation_0+((theta_array[0]+theta_array[1]*x[i])-y[i])
    summation_1=summation_1+x[i]*((theta_array[0]+theta_array[1]*x[i])-y[i])

  new_theta0=theta_array[0]-(alpha/m)*summation_0
  new_theta1=theta_array[1]-(alpha/m)*summation_1
  updated_new_theta=[new_theta0,new_theta1]
  return updated_new_theta


def training(x,y,alpha,itres):
  theta0=0
  theta1=0
  theta_array=[theta0,theta1]
  cost_value=[]
  m=x.size
  for i in range(itres):
    theta_array=gradient_descent(theta_array,x,y,m,alpha)
    cost_value.append(cost_function(theta_array,x,y,m))
    y_predict=x*theta_array[1]+theta_array[0]
    plt.plot(x,y_predict,'r')
    plt.scatter(x,y)
    plt.show()
  return cost_value, theta_array

#feeding the input data
training_data=data.dropna()

training_data.shape

x_value=training_data['x']
y_value=training_data['y']

type(x_value)

x_value=x_value.values.reshape(x_value.size)
y_value=y_value.values.reshape(y_value.size)

type(x_value)

alpha=0.0001
itres=50
cost_val,theta_array=training(x_value,y_value,alpha,itres)
x_axis=np.arange(0,len(cost_val),step=1)
plt.plot(x_axis,cost_val)
plt.xlabel("ites")
plt.ylabel("Cost_val")
plt.title("Loss graph")
plt.show()
print(theta_array)
hypothesis(theta_array,54.0)

""" HomeWork
observe the loss function curve for
alpha=0.000000001,0.000001,0.01,1,5,10,100
also take different 10 values of interations
"""

# Homework: observe the loss function curve for different alphas and iterations
alpha_values = [0.000000001, 0.000001, 0.01, 1, 5, 10, 100]
itres_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

for alpha in alpha_values:
    for itres in itres_values:
        print(f"Training with alpha={alpha} and iterations={itres}")
        cost_val,final_theta = training(x_value, y_value, alpha, itres)
        x_axis = np.arange(0, len(cost_val), step=1)
        plt.plot(x_axis, cost_val, label=f'alpha={alpha}, iters={itres}')
        plt.xlabel("Iterations")
        plt.ylabel("Cost_val")
        plt.title("Loss graph")
        plt.legend()
        plt.show()
        print(f"Final theta for alpha={alpha}, iterations={itres}: {final_theta}")
        # You can also calculate and print the predicted value for a sample x if needed
        predicted_y = hypothesis(final_theta, 54.0)
        # print(f"Predicted y for x=54.0: {predicted_y}")

import numpy as np
import matplotlib.pyplot as plt

# Define the training function (example template)
def training(x, y, alpha, iterations):
    m = len(y)
    theta = np.zeros(x.shape[1])
    cost_values = []

    for i in range(iterations):
        prediction = np.dot(x, theta)
        error = prediction - y
        cost = (1 / (2 * m)) * np.dot(error.T, error)
        cost_values.append(cost)
        gradient = (1 / m) * np.dot(x.T, error)
        theta = theta - alpha * gradient

    return cost_values, theta  # âœ… MUST return two values

# Sample input data (replace with your actual data)
x_value = np.array([[1, i] for i in range(100)])  # 2D input with bias term
y_value = np.array([2 * i + 1 for i in range(100)])  # y = 2x + 1

# Alpha and iteration settings
alpha_values = [0.000000001, 0.000001, 0.01, 1, 5, 10, 100]
itres_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

# Loop through combinations and plot
for alpha in alpha_values:
    for itres in itres_values:
        print(f"Training with alpha={alpha} and iterations={itres}")
        cost_val, final_theta = training(x_value, y_value, alpha, itres)
        x_axis = np.arange(0, len(cost_val), step=1)
        plt.plot(x_axis, cost_val, label=f'alpha={alpha}, iters={itres}')
        plt.xlabel("Iterations")
        plt.ylabel("Cost_val")
        plt.title("Loss graph")
        plt.legend()
        plt.show()
        print(f"Final theta for alpha={alpha}, iterations={itres}: {final_theta}")

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

colnames=['area','rooms','price']
dataset = pd.read_csv("https://raw.githubusercontent.com/nishithkotak/machine-learning/master/ex1data2.txt",names=colnames)

dataset

dataset.describe()

area=dataset.iloc[0:dataset.shape[0],0:1]
rooms=dataset.iloc[0:dataset.shape[0],1:2]
price=dataset.iloc[0:dataset.shape[0],2:3]

dataset.shape

# function of normalization
def feature_normalization(x):
    mean=np.mean(x,axis=0)
    std=np.std(x,axis=0)
    x_normalized=(x-mean)/std
    return x_normalized,mean,std

data_norm=dataset.values
m=data_norm.shape[0]
# take the feature vectors
x2=data_norm[:,0:2].reshape(m,2)
x2_norm,mean,std=feature_normalization(x2)

y2=data_norm[:,2:3].reshape(m,1)#price column

x2_norm

theta_array=np.zeros((3,1))

#Linear Regression for 3 variables
# def hypothesis_3_vars(theta_array, x):
#   return theta_array[0] + theta_array[1] * x[:, 0] + theta_array[2] * x[:, 1]

# def cost_function_3_vars(theta_array, x, y, m):
#   error = hypothesis_3_vars(theta_array, x) - y.flatten()
#   return (1 / (2 * m)) * np.sum(error ** 2)

# def gradient_descent_3_vars(theta_array, x, y, m, alpha):
#   error = hypothesis_3_vars(theta_array, x) - y.flatten()
#   summation_0 = np.sum(error)
#   summation_1 = np.sum(error * x[:, 0])
#   summation_2 = np.sum(error * x[:, 1])

#   new_theta0 = theta_array[0] - (alpha / m) * summation_0
#   new_theta1 = theta_array[1] - (alpha / m) * summation_1
#   new_theta2 = theta_array[2] - (alpha / m) * summation_2
#   updated_new_theta = np.array([new_theta0, new_theta1, new_theta2])
#   return updated_new_theta

# def training_3_vars(x, y, alpha, itres):
#   theta = np.zeros(3) # Initialize theta with 3 values
#   cost_value = []
#   m = x.shape[0]
#   for i in range(itres):
#     theta = gradient_descent_3_vars(theta, x, y, m, alpha)
#     cost_value.append(cost_function_3_vars(theta, x, y, m))

#   return cost_value, theta

#linear regression
def hypothesis(theta_array,x1,x2):
  return theta_array[0]+theta_array[1]*x1 +theta_array[2]*x2

def cost_funtion(theta_array,x1,x2,y,m):
  sum=0
  for i in range(m):
    sum=sum+((theta_array[0]+theta_array[1]*x1[i]+theta_array[2]*x2[i])-y[i])*2

  return sum/(2*m)

def gradiant_descent(theta_array,x1,x2,y,m,alpha):
  summation_0=0
  summation_1=0
  summation_2=0
  for i in range(m):
    summation_0=summation_0+((theta_array[0]+theta_array[1]*x1[i]+theta_array[2]*x2[i])-y[i])
    summation_1=summation_1+x1[i]*((theta_array[0]+theta_array[1]*x1[i]+theta_array[2]*x2[i])-y[i])
    summation_2=summation_2+x2[i]*((theta_array[0]+theta_array[1]*x1[i]+theta_array[2]*x2[i])-y[i])
  new_theta0=theta_array[0]-(alpha/m)*summation_0
  new_theta1=theta_array[1]-(alpha/m)*summation_1
  new_theta2=theta_array[2]-(alpha/m)*summation_2
  updated_new_theta=[new_theta0,new_theta1,new_theta2]
  return updated_new_theta

def training(x1,x2,y,alpha,iters,theta_array,m):

  cost_values=[]

  for i in range(iters):
    theta_array=gradiant_descent(theta_array,x1,x2,y,m,alpha)
    cost_values.append(cost_funtion(theta_array,x1,x2,y,m))

  return cost_values,theta_array

alpha=0.001
iters=500

cost_values,theta_array=training(x2_norm[:,0:1].flatten(),x2_norm[:,1:2].flatten(),y2.flatten(),alpha,iters,theta_array,m)

cost_values,theta_array=training(x2_norm[:,0:1],x2_norm[:,1:2],y2,alpha,itres,theta_array,m)
x_axis=np.arange(0,len(cost_values[0]),step=1)
plt.plot(x_axis,cost_values[0])
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.title("show")
plt.show()
print(theta_array)

