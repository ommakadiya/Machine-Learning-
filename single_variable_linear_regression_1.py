# -*- coding: utf-8 -*-
"""Single Variable Linear Regression-1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WwgkJldMVkGlWz4wN1VdjNq04AHqZCxO
"""

import pandas as pd
df = pd.read_csv('exp-1_train.csv')
df.describe()

df.head()

df.shape

x=df.iloc[0:700,0:1]
y=df.iloc[0:700,1:2]

x

x.boxplot(column=('x'))

y.boxplot(column=('y'))

#plot the scater plot
import matplotlib.pyplot as plt
plt.scatter(x,y)
plt.xlabel('x')
plt.ylabel('y')
plt.title("X vs Y Scatter Plot")

#Linear regression
def hypothesis(theta_array,x):
  return theta_array[0]+theta_array[1]*x

from sre_constants import error
def cost_function(theta_array,x,y,m):
  error=0
  for i in range(m):
    error=error+(theta_array[0]+theta_array[1]*x[i])-y[i]**2
  return error/(2*m)
  # return (1/(2*m))*sum((hypothesis(theta_array,x)-y)**2)

def gradient_descent(theta_array,x,y,m,alpha):
  sum_0=0
  sum_1=0
  for i in range(m):
    sum_0=sum_0+((theta_array[0]+theta_array[1]*x[i])-y[i])
    sum_1=sum_1+((theta_array[0]+theta_array[1]*x[i])-y[i])*x[i]
  new_theta0=theta_array[0]-(alpha/m)*sum_0
  new_theta1=theta_array[1]-(alpha/m)*sum_1
  update_new_theta=[new_theta0,new_theta1]
  return update_new_theta

def training(x,y,alpha,itres):
  theta1=0
  theta0=0
  cost_value=[]
  theta_array=[theta0,theta1]
  m=x.size
  for i in range(itres):
    theta_array=gradient_descent(theta_array,x,y,m,alpha)
    cost_value.append(cost_function(theta_array,x,y,m))
    y_predict=x*theta_array[1]+theta_array[0]
    plt.plot(x,y_predict,'r')
    plt.scatter(x,y)
    plt.show()
  return cost_value, theta_array

#feeding the input data
training_data=df.dropna()
training_data.shape

x_value=training_data['x']
y_value=training_data['y']

x_value=x_value.values
y_value=y_value.values
type(x_value)

import numpy as np
import matplotlib.pyplot as plt
alpha=0.0001
itres=50
cost_values,theta_array=training(x_value,y_value,alpha,itres)
x_axis=np.arange(0,len(cost_values),step=1)
plt.plot(x_axis,cost_values)
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.title("show")
plt.show()
print(theta_array)

hypothesis(theta_array,25)

import numpy as np
import matplotlib.pyplot as plt
alpha = [0.000000001, 0.000001, 0.01, 1, 5, 10, 100]
itres = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
cost_values, theta_array = training(x_value,y_value,alpha,itres)
x_axis = np.arange(0, len(cost_values), step=1)
plt.plot(x_axis, cost_values)
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.title("Gradient Descent Cost over Iterations")
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Define the training function (example template)
def training(x, y, alpha, iterations):
    m = len(y)
    theta = np.zeros(x.shape[1])
    cost_values = []

    for i in range(iterations):
        prediction = np.dot(x, theta)
        error = prediction - y
        cost = (1 / (2 * m)) * np.dot(error.T, error)
        cost_values.append(cost)
        gradient = (1 / m) * np.dot(x.T, error)
        theta = theta - alpha * gradient

    return cost_values, theta  # âœ… MUST return two values

# Sample input data (replace with your actual data)
x_value = np.array([[1, i] for i in range(100)])  # 2D input with bias term
y_value = np.array([2 * i + 1 for i in range(100)])  # y = 2x + 1

# Alpha and iteration settings
alpha_values = [0.000000001, 0.000001, 0.01, 1, 5, 10, 100]
itres_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

# Loop through combinations and plot
for alpha in alpha_values:
    for itres in itres_values:
        print(f"Training with alpha={alpha} and iterations={itres}")
        cost_val, final_theta = training(x_value, y_value, alpha, itres)
        x_axis = np.arange(0, len(cost_val), step=1)
        plt.plot(x_axis, cost_val, label=f'alpha={alpha}, iters={itres}')
        plt.xlabel("Iterations")
        plt.ylabel("Cost_val")
        plt.title("Loss graph")
        plt.legend()
        plt.show()
        print(f"Final theta for alpha={alpha}, iterations={itres}: {final_theta}")

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

columnames=['area','rooms','price']
dataset=pd.read_csv('https://raw.githubusercontent.com/nishithkotak/machine-learning/refs/heads/master/ex1data2.txt',names=columnames)
dataset

dataset.describe()

area=dataset.iloc[0:dataset.shape[0],0:1]
rooms=dataset.iloc[0:dataset.shape[0],1:2]
price=dataset.iloc[0:dataset.shape[0],2:3]

dataset.shape

#funcation normalization
def feature_normalization(x):
  meaan=np.mean(x,axis=0)
  std=np.std(x,axis=0)
  x_normalized =(x-meaan)/std
  return x_normalized,meaan,std

data_norm=dataset.values
m=data_norm.shape[0]
#taking feature vector

x2=data_norm[:,0:2].reshape(m,2)
x2_norm,mean,std=feature_normalization(x2)

y2=data_norm[:,2:3].reshape(m,1)
x2_norm

theta_array=np.zeros((3,1))

def hypothesis(theta_array,x2_norm):
  return x2_norm[0]+theta_array[2]*x2_norm[1]

def cost_funcation(theta_array,x2_norm,y2,m):
  error=0
  for i in range(m):
    error=error+((x2_norm[0]+theta_array[2]*x2_norm[1])-y2[i])**2
    return error/(2*m)

# def gradient_descent(theta_array,x,y,m,alpha):
#   sum_0=0
#   sum_1=0
#   for i in range(m):
#     sum_0=sum_0+((theta_array[0]+theta_array[1]*x[i])-y[i])
#     sum_1=sum_1+((theta_array[0]+theta_array[1]*x[i])-y[i])*x[i]
#   new_theta0=theta_array[0]-(alpha/m)*sum_0
#   new_theta1=theta_array[1]-(alpha/m)*sum_1
#   update_new_theta=[new_theta0,new_theta1]
#   return update_new_theta
def gradient_descent(theta_array,x2_norm,y2,m,alpha):
  sum_0=0
  sum_1=0
  for i in range(m):
    sum_0=sum_0+(((x2_norm[0]+theta_array[2]*x2_norm[1])-y2[i]))
    sum_1=sum_1+(((x2_norm[0]+theta_array[2]*x2_norm[1])-y2[i])*x2_norm[i])
    sum_2=sum_2+((x2_norm[0]+theta_array[2]*x2_norm))
  new_theta0=theta_array[0]-(alpha/m)*sum_0
  new_theta1=theta_array[1]-(alpha/m)*sum_1
  new_theta2=theta_array[2]-(alpha/m)*sum_2
  update_new_theta=[new_theta0,new_theta1,new_theta2]
  return update_new_theta

