# -*- coding: utf-8 -*-
"""ML Assignment_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h8Q9zfTbSqo2hS4bq9HFakVe3WaWIJNN
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

np.random.seed(0)
num_samples = 50

df = pd.DataFrame({
    "area": np.random.randint(low=800, high=3000, size=num_samples),
    "rooms": np.random.randint(1, 5, size=num_samples),
    "age": np.random.randint(1, 20, size=num_samples)
})

df["price"] = (
    df["area"] * 200
    + df["rooms"] * 15000
    - df["age"] * 800
    + np.random.normal(loc=0, scale=20000, size=num_samples)
)

# Regression Classification
price_bins = [0, 5e5, 9e5, float("inf")]
price_labels = [0, 1, 2]
df["price_class"] = pd.cut(df["price"], bins=price_bins, labels=price_labels).astype(int)

features = df[["area", "rooms", "age"]]
target = df["price_class"]
num_classes = target.nunique()

#training and testing sets in split data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.2, random_state=42, stratify=target
)

# Convert DataFrame to NumPy arrays for numerical operations
train_X = X_train.to_numpy()
train_y = y_train.to_numpy()
test_X = X_test.to_numpy()
test_y = y_test.to_numpy()

# Logistic (sigmoid) activation function
def logistic_fn(x):
    return 1.0 / (1.0 + np.exp(-x))

# One-vs-Rest logistic regression training with gradient descent
def fit_ovr(train_X, train_y, num_classes, learning_rate=1e-7, iterations=10000):
    samples, features = train_X.shape
    model_params = []

    for cls in range(num_classes):
        # Create binary target for current class
        binary_target = (train_y == cls).astype(float)
        weights = np.zeros(features)
        bias = 0.0

        for _ in range(iterations):
            linear_output = np.dot(train_X, weights) + bias
            predictions = logistic_fn(linear_output)

            # Calculate gradients
            grad_w = np.dot(train_X.T, (predictions - binary_target)) / samples
            grad_b = np.mean(predictions - binary_target)

            # Parameter update
            weights -= learning_rate * grad_w
            bias -= learning_rate * grad_b

        model_params.append({"weights": weights, "bias": bias})

    return model_params

# Prediction function
def classify(X_input, params):
    n_obs = X_input.shape[0]
    total_classes = len(params)
    class_probs = np.zeros((n_obs, total_classes))

    for cls_idx in range(total_classes):
        w = params[cls_idx]["weights"]
        b = params[cls_idx]["bias"]
        score = np.dot(X_input, w) + b
        class_probs[:, cls_idx] = logistic_fn(score)

    return np.argmax(class_probs, axis=1)

#Train from scratch
params = fit_ovr(train_X, train_y, num_classes)

y_predictions = classify(test_X, params)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print("Accuracy:", accuracy_score(test_y, y_predictions))
print("\nClassification Report:\n", classification_report(test_y, y_predictions))
print("\nConfusion Matrix:\n", confusion_matrix(test_y, y_predictions))

# 6. Sample Predictions Table
sample_results = X_test.copy()
sample_results['true_price'] = df.loc[X_test.index, 'price']
sample_results['true_class'] = y_test
sample_results['pred_class'] = y_predictions
print(sample_results.head())

# Class distribution
print("\nClass distribution:")
print(df['price_class'].value_counts())

